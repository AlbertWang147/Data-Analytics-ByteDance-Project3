{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# In[]\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "\n",
    "# In[]\n",
    "class Corpus(object):\n",
    "    def __init__(self, data):\n",
    "        self.tags = defaultdict(int)\n",
    "        self.vocabs = set()\n",
    "        self.docs = []\n",
    "        \n",
    "        self.build_vocab(data)\n",
    "        self.v_l = len(self.vocabs) # 字典的大小\n",
    "        self.d_l = len(self.docs)   # 文档数\n",
    "    \n",
    "    # 分词器\n",
    "    def tokenizer(self, sent):\n",
    "        return jieba.lcut(sent)\n",
    "\n",
    "    # 构建字典，获取分类标记集\n",
    "    def build_vocab(self, data):\n",
    "        for (tag, doc, doc2) in data:\n",
    "            words = self.tokenizer(doc)        \n",
    "            self.vocabs.update(words)\n",
    "            self.tags[tag] += 1\n",
    "            \n",
    "            words2 = self.tokenizer(doc2)\n",
    "            self.vocabs.update(words2)\n",
    "            \n",
    "            words = words + words2\n",
    "            self.docs.append((tag, words, words2))\n",
    "        self.vocabs = list(self.vocabs)\n",
    "    \n",
    "    # 计算词袋模型\n",
    "    def calc_bow(self):\n",
    "        # shape为 [d_l, v_l]，每一行存放文档的词袋向量\n",
    "        self.bow = np.zeros([self.d_l, self.v_l])\n",
    "        for idx in range(self.d_l):\n",
    "            for word in self.docs[idx][1]:\n",
    "                if word in self.vocabs:\n",
    "                    self.bow[idx, self.vocabs.index(word)] += 1\n",
    "    \n",
    "    # 计算tf-idf\n",
    "    def calc_tfidf(self):\n",
    "        # 先计算bow，再用bow来计算tf\n",
    "        self.calc_bow()\n",
    "        \n",
    "        # 初始化tf、df、idf\n",
    "        self.tf = np.zeros([self.d_l, self.v_l])\n",
    "        self.idf = np.ones([1, self.v_l])\n",
    "        self.tf_idf = np.ones([self.d_l, self.v_l])\n",
    "        for idx in range(self.d_l):\n",
    "            self.tf[idx] = self.bow[idx] /np.sum(self.bow[idx])\n",
    "            for word in self.docs[idx]:\n",
    "                if word in self.vocabs:\n",
    "                    self.idf[0, self.vocabs.index(word)] += 1\n",
    "        self.idf = np.log(float(self.d_l) / self.idf)\n",
    "        self.tfidf = self.tf * self.idf\n",
    "        \n",
    "    # 计算输入的bow向量，words代表输入序列（已分词）\n",
    "    def get_idx(self, words):\n",
    "        bow = np.zeros([1, self.v_l])\n",
    "        for word in words:\n",
    "            if word in self.vocabs:\n",
    "                bow[0, self.vocabs.index(word)] += 1\n",
    "        return bow\n",
    "\n",
    "# NB继承了语料类Corpus\n",
    "class NBayes(Corpus):\n",
    "    def __init__(self, data, kernel=\"tfidf\"):\n",
    "        super(NBayes, self).__init__(data)\n",
    "    \n",
    "        # kernel 代表使用哪种特征，默认是tfidf，赋其他值代表使用bow\n",
    "        self.kernel = kernel\n",
    "        self.y_prob = {} # p(y_i)\n",
    "        self.c_prob = None # p(x|y_i) , 计算条件概率\n",
    "        self.feature = None\n",
    "    \n",
    "    # 训练，主要计算 p(y_i)和条件概率 p(x|y_i)\n",
    "    def train(self):\n",
    "        if self.kernel == \"tfidf\":\n",
    "            self.calc_tfidf()\n",
    "            self.feature = self.tfidf\n",
    "        else:\n",
    "            self.calc_bow()\n",
    "            self.feature = self.bow\n",
    "    \n",
    "        # 采用极大似然估计计算p(y)\n",
    "        for tag in self.tags:\n",
    "            self.y_prob[tag] = float(self.tags[tag])/ self.d_l\n",
    "\n",
    "        # 计算条件概率 p(x|y_i)\n",
    "        self.c_prob = np.zeros([len(self.tags), self.v_l])\n",
    "        Z = np.zeros([len(self.tags), 1]) # 归一化参数\n",
    "\n",
    "        for idx in range(self.d_l):\n",
    "            # 获得类别标签id\n",
    "            tid = list(self.tags.keys()).index(self.docs[idx][0])\n",
    "            self.c_prob[tid] += self.feature[idx]\n",
    "            Z[tid] = np.sum(self.c_prob[tid])\n",
    "\n",
    "        self.c_prob /= Z  # 归一化\n",
    "    \n",
    "    # 解码部分，返回使得概率值最大的类别y\n",
    "    def predict(self, inp):\n",
    "        words = self.tokenizer(inp[0])\n",
    "        words2 = self.tokenizer(inp[1])\n",
    "        words = words + words2\n",
    "        idx = self.get_idx(words)\n",
    "\n",
    "        tag, score = None, -1\n",
    "        for (p_c, y) in zip(self.c_prob, self.y_prob):\n",
    "            tmp = np.sum(idx * p_c * self.y_prob[y])\n",
    "\n",
    "            if tmp > score:\n",
    "                tag = y\n",
    "                score = tmp\n",
    "        return tag, 1.0 - score\n",
    "\n",
    "# In[]\n",
    "path_to_file = r'data.xlsx'\n",
    "data = pd.read_excel(path_to_file)\n",
    "\n",
    "x_index = ['视频标题', '视频描述']\n",
    "y_index = '视频分区'\n",
    "\n",
    "# 建立并训练模型\n",
    "trainSet = [(data[y_index][i], data[x_index[0]][i], data[x_index[1]][i]) for i in range(data.shape[0])]\n",
    "nb = NBayes(trainSet)\n",
    "nb.train()\n",
    "\n",
    "# 预测\n",
    "i = 3\n",
    "print(nb.predict((data[x_index[0]][i], data[x_index[1]][i]))) # ('生活', 0.9721141084848975)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
